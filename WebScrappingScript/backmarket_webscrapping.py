# -*- coding: utf-8 -*-
""" 
This is a WebScrapping script I had to do for a task in the specialization course I did, the goal was to scrap a website and obtain as many features as possible
from the products of the website, we also had to scrap different categories and for each category scrap every page it had.

We also had to catch the errors and send an email where we could see if the scrap was completed succesfully or not.

The only problem I found doing this script was to scrap automatically all the pages that a category had. I finally did it manually by adjusting the number of pages of each category
and adapting that number in the loop.
"""

# Import libraries
import requests
import hashlib
from bs4 import BeautifulSoup
import time
import csv
import logging
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

# Logs configuraton to save if the script was executed succesfullly or not in a file named 'scrapig_log.log'
logging.basicConfig(
    filename="scraping_log.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# Dictionary where we store the different categories and the base URL that we are going to scrap
categories = {
    'MacBooks': 'https://www.backmarket.es/es-es/l/macbook/297f69c7-b41c-40dd-aa9b-93ab067eb691',
    'Portátiles Ofimáticos': 'https://www.backmarket.es/es-es/l/portatiles-ofimaticass/7684cc26-1f7e-4eff-9324-905ab7663162',
    'Móviles': 'https://www.backmarket.es/es-es/l/smartphoness/6c290010-c0c2-47a4-b68a-ac2ec2b64dca',
    'Smartwatches': 'https://www.backmarket.es/es-es/l/relojes-conectadoss/0894adca-7735-40d3-a34b-5a77358e3937'
}

# Dict to store how many pages each category has. We will use it to iterate over them later
pages_per_category = {
    'MacBooks': 20,
    'Portátiles Ofimáticos': 34,
    'Móviles': 34,
    'Smartwatches': 20
}

# Assing the values to the variables
EMAIL_ADDRESS = "REPLACE THIS WITH THE EMAIL YOU ARE GOING TO SEND THE MESSAGE FROM"
EMAIL_PASSWORD =  "REPLACE THIS WITH THE PASSWORD THAT GMAIL GENERATED"
EMAIL_RECIPIENT =  'REPLACE THIS WITH THE EMAIL THAT IS GOING TO RECEIVE THE MESSAGE'

# Function to send the email
def send_email(success, message=""):
    """
    Send an email saying wether thescript was completed succesfully or nor.

    Args:
        success (bool): Says if the script ended ok or not.
        message (str): Additional message saying the error.
    """
    subject = "Scraping completed succesfully" if success else "The scrap wasn't completed succesfully, check it"
    body = (
        "The script was completed succesfully and the data has been stored in the CSV file"
        if success
        else f"The script failed:\n\n{message}"
    )
    #Configuration of the email content
    msg = MIMEMultipart()
    msg["From"] = EMAIL_ADDRESS
    msg["To"] = EMAIL_RECIPIENT
    msg["Subject"] = subject
    msg.attach(MIMEText(body, "plain"))

    # Try to send the email and store in the log if it was succesful, otherwise save the error to check it later
    try:
        with smtplib.SMTP("smtp.gmail.com", 587) as server:
            server.starttls()
            server.login(EMAIL_ADDRESS, EMAIL_PASSWORD)
            server.send_message(msg)
        logging.info("Email sent.")
    except Exception as e:
        logging.error(f"Error sending the email: {e}")


# Function to generate a unique id for every item, this is made to be able to represent the variance of the price among the time later in the database for each product
# So in case the webpage delete de item we will know until when it was available and the price it had at that time
# I create the function because I didn't find the product ID inside the website at the time I did the project
def generate_unique_id(name, url):
    """
    Generate a unique for a product.

    The identifier is generated by creating an MD5 hash of the combination
    of the product name and its URL or image URL. This ensures we can identify
    the same product in the differente CSV that are going to be generated

    Args:
        name (str): The name of the product.
        url (str): The URL or image URL of the product.

    Returns:
        str: A 32-character hexadecimal string representing the unique identifier.
    """
    unique_string = f"{name}-{url}"
    return hashlib.md5(unique_string.encode()).hexdigest()

# Main Script
try:
    # Actual date saved in 'today' and creation of the csv file where the data is going to be stored
    today = time.strftime('%Y%m%d')
    my_scrap = f'{today}_Backmarket_Roberto_Martin_Romo_Valderrama.csv'

    # Open the file and write the header
    with open(my_scrap, 'w', newline='', encoding='utf-16') as file_csv: # Open the file as file_csv
            # Create a writter csv object
            csv_writter = csv.DictWriter(file_csv, fieldnames=['Page', 'Name', 'Price', 'Badge', 'GBs', 'Reviews', 'Rating', 'Date', 'Image', 'ID'])
            # Write the headers
            csv_writter.writeheader()

            # Iterate over every category in the dict and assign the number of pages to 'total_pages'
            for category_name, base_url in categories.items():
                total_pages = pages_per_category[category_name]
                print(f'Scrapeando categoría: {category_name}')

                # For every page of each category create the url and start scraping it
                for i in range(total_pages + 1):
                    print(f'Scrapeando página {i} de {category_name}...')
                    url = f'{base_url}?p={i}'
                    response = requests.get(url)

                    # If the status code is correct start the scrapping
                    if response.status_code == 200:
                        # Create BeautifulSoup Object and extract every article in the webpage
                        soup = BeautifulSoup(response.text, 'html.parser')
                        articles = soup.find_all('div', class_='flex grow flex-wrap content-start justify-center gap-8')

                        # In case there are no articles stop the script and write a warning in the log file
                        if not articles:
                            logging.warning(f'No articles found on page {i + 1} of {category_name}')
                            break

                        # Iteration to find everything we want of every article
                        for article in articles:
                            try:
                                # Get the name of the product
                                name = article.find('span', class_='body-1-bold line-clamp-2').text.strip()

                                # Get the price and badge and split it into 2 differente variables 'price' and 'badge'
                                price_and_badge = article.find('div', class_='text-static-default-hi body-2-bold').text.strip()
                                price = price_and_badge.split()[0].replace(',', '.')
                                badge = price_and_badge.split()[1]

                                # Get the image link and join it with the website link to obtain the absolut link
                                img = 'https://www.backmarket.es' + article.find('img')['src']

                                # Get how many reviews the product has
                                reviews_tag = article.find('span', class_='text-static-default-hi caption ml-4')
                                verified_reviews = reviews_tag.text.strip() if reviews_tag else 'No reviews'

                                # For each product, in case there are different storage sizes, get them all
                                gb_list = []
                                disk_space_tag = article.find('ul', class_='flex list-none flex-wrap items-center gap-4')
                                if disk_space_tag:
                                    gb_list = [item.strip() for item in disk_space_tag.text.split('GB') if item.strip()]
                                gb_availability = [f'{value} GB' for value in gb_list] if gb_list else 'N/A'

                                # Obtain the rating of the article
                                rating_tag = article.find('span', class_='ml-4 mt-1 md:mt-2 caption-bold')
                                rating = rating_tag.text.strip() if rating_tag else 'No rating'

                                # Generate the id using the function mentioned above
                                unique_id = generate_unique_id(name, img)

                                # Write a row for each product to visualize the information scrapped
                                csv_writter.writerow({'Page': i + 1,
                                                        'Name': name,
                                                        'Price': price,
                                                        'Badge': badge,
                                                        'GBs': gb_availability,
                                                        'Reviews': verified_reviews,
                                                        'Rating': rating,
                                                        'Date': today,
                                                        'Image': img,
                                                        'ID': unique_id
                                                        })

                            # In case there is an error write it in the log page to visualize it
                            except Exception as e:
                                logging.error(f'Error prcessing article on page {i + 1} of cateogry {category_name}: {e}')
                                continue

                    # In case the status_code is different than 200 print the status code
                    else:
                        print(f'Error {response.status_code} on page {i} of {category_name}')
                    # Sleep 1 second over each iteration
                    time.sleep(1)

    # In case everything is ok send the email described above
    send_email(success=True)

# If there is any error on the execution save it on the log file and send an email saying it, otherwise save a positive string
except Exception as e:
    logging.error(f'Fail on the execution of the script: {e}')
    send_email(success=False)
else:
    logging.info('The script was completed succesfully.')