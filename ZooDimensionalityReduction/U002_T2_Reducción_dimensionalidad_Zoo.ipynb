{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB7CYwBSHfIW"
   },
   "source": [
    "In this project I was working with the zoo dataset https://www.kaggle.com/uciml/zoo-animal-classification?select=zoo.csv .\n",
    "This dataset provides information about different features of animals and classifies them on different classes.\n",
    "The objectives was to analyze it and apply different techniques of dimensionality reduction to see the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and see what it contains\n",
    "df = pd.read_csv('zoo.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCCmvKV_HT6G"
   },
   "source": [
    "#2. Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'animal_name' as it usseles for our task\n",
    "df.drop(['animal_name'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check statistical data \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2SyzP0kHL8O"
   },
   "source": [
    "#3. Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEocFWERHINe"
   },
   "source": [
    "#4. Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVdeHhscHCXG"
   },
   "source": [
    "#5. Correlation. Which are the most correlated features with the target ? That's what I'm going to check now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target from the features, save features in 'X' and target in 'y'\n",
    "X = df.drop('class_type', axis=1)\n",
    "y = df['class_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to calculate the correlation using Pearson's method.\n",
    "cor = df.corr()\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we got the correlation table save the absolute values in 'cor_target' variable and check which of them are above 0.5 \n",
    "cor_target = abs(cor[\"class_type\"])\n",
    "\n",
    "# Our threshold for this task is 0.5 so we will say that every feature that is above 0.5 of correlation is highly correlated with the target.\n",
    "relevant_features = cor_target[cor_target>0.5]\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNM-a-oCG9U2"
   },
   "source": [
    "#6. Which feature has the maximum correlation value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cor = relevant_features.drop('class_type').idxmax()\n",
    "max_cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying Pearson's method we can see that the most correlated feature is 'backbone' with a value of 0.828845."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QASrb86yG1EH"
   },
   "source": [
    "#7. Now let's apply Spearman's method to check if there is any difference with the method we applied previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Spearman's method\n",
    "spearman_corr = df.corr(method='spearman')\n",
    "spearman_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a heatmap to see every correlation value between the features but I'm not doing that in this project.\n",
    "\n",
    "# Let's use the same code than before to check which features values are above 0.5\n",
    "spearman_cor_target = abs(spearman_corr['class_type'])\n",
    "\n",
    "features = spearman_cor_target[spearman_cor_target > 0.5]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can see that now the feature with the max correlation value is 'milk' with a value of 0.886024\n",
    "max_cor_spearman = features.drop('class_type').idxmax()\n",
    "max_cor_spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using both methods, the correlations change.\n",
    "\n",
    "Using the Pearson method, the variable with the highest correlation is 'backbone,' while using Spearman, it is 'milk.' Additionally, the correlations of the other variables differ from one method to another.\n",
    "\n",
    "This is because the Pearson method measures the linear correlation between variables, that is, how well they align with a direct linear relationship. This method is sensitive to the scale of values and the presence of outliers.\n",
    "\n",
    "On the other hand, the Spearman method measures the correlation of variables with ordinal data, assessing whether a relationship exists even when the data is non-linear. It uses the rank of the values instead of the values themselves, making it less sensitive to outliers and non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XwTOZKeOuaT"
   },
   "source": [
    "#8. Now let's work with backward elimination. Which features do we keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply backward elimination we are going to use the OLS algorithm (Ordinary Least Squares), this method uses the p_value to identify which columns has to keep to train the model\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "cols = list(X.columns)\n",
    "X_1 = X[cols]\n",
    "X_1 = sm.add_constant(X_1)\n",
    "model = sm.OLS(y,X_1).fit()\n",
    "model.summary()\n",
    "# When "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now there's the implementation of the backaward algorithm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "cols = list(X.columns)\n",
    "pmax = 1\n",
    "while (len(cols)>0):\n",
    "    p = []\n",
    "    X_1 = X[cols]\n",
    "    X_1 = sm.add_constant(X_1)\n",
    "    model = sm.OLS(y,X_1).fit()\n",
    "    p = pd.Series(model.pvalues.values[1:],index = cols)\n",
    "    pmax = max(p)\n",
    "    feature_with_p_max = p.idxmax()\n",
    "    if(pmax>0.05):\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "selected_features_BE = cols\n",
    "print(selected_features_BE)\n",
    "\n",
    "\n",
    "\"\"\" As we can see in the output now we have 7 features left of the 17 we had at the beginning\n",
    "['feathers', 'milk', 'airborne', 'aquatic', 'toothed', 'backbone', 'fins'] \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57yhfdZPO-zn"
   },
   "source": [
    "#9. Which is the optimum number of features when applying RFE? Which are them ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RFE algorithm classifies the columns from most to least important instead of using the p_value\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a Lienar Regression model and select 8 features\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rfe = rfe.fit(X,y)\n",
    "X_rfe = rfe.transform(X)\n",
    "model.fit(X_rfe,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training the columns that appear with a 1 are the most important ones\n",
    "\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets check the optimal number and the score when using those columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#no of features\n",
    "nof_list=np.arange(1,17)\n",
    "high_score=0\n",
    "#Variable to store the optimum features\n",
    "nof=0\n",
    "score_list =[]\n",
    "for n in range(len(nof_list)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
    "    model = LinearRegression()\n",
    "    rfe = RFE(model, n_features_to_select=nof_list[n])\n",
    "    X_train_rfe = rfe.fit_transform(X_train,y_train)\n",
    "    X_test_rfe = rfe.transform(X_test)\n",
    "    model.fit(X_train_rfe,y_train)\n",
    "    score = model.score(X_test_rfe,y_test)\n",
    "    score_list.append(score)\n",
    "    if(score>high_score):\n",
    "        high_score = score\n",
    "        nof = nof_list[n]\n",
    "print(\"Optimum number of features: %d\" %nof)\n",
    "print(\"Score with %d features: %f\" % (nof, high_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last let's check the name of the columns we worked with\n",
    "cols = list(X.columns)\n",
    "model = LinearRegression()\n",
    "#Initializing RFE model\n",
    "rfe = RFE(model, n_features_to_select=3)\n",
    "#Transforming data using RFE\n",
    "X_rfe = rfe.fit_transform(X,y)\n",
    "#Fitting the data to model\n",
    "model.fit(X_rfe,y)\n",
    "temp = pd.Series(rfe.support_,index = cols)\n",
    "selected_features_rfe = temp[temp==True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the name of the most optimal columns\n",
    "selected_features_rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxVu2kJcPcB5"
   },
   "source": [
    "#10. Now lets work with RIDGE and LASSO, let's check how many features each of the algorithm drops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly let's check which variables RIDGE drops, this algorithm penalize the components to not overfit and makes the feature selection during training\n",
    "\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reg_ridge = RidgeCV()\n",
    "reg_ridge.fit(X, y)\n",
    "coef = pd.Series(reg_ridge.coef_, index=X.columns)\n",
    "\n",
    "features_chosen = coef[coef != 0].index.tolist()\n",
    "features_dropped = coef[coef == 0].index.tolist()\n",
    "\n",
    "print(\"Ridge chose \" + str(sum(coef != 0)) + \" features and dropped \" +  str(sum(coef == 0)) + \" features\")\n",
    "print('Features chosen', features_chosen)\n",
    "print('Features dropped', features_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Importance of variables using the Ridge Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lasso = LassoCV()\n",
    "reg_lasso.fit(X, y)\n",
    "coef_2 = pd.Series(reg_lasso.coef_, index=X.columns)\n",
    "\n",
    "features_chosen_2 = coef_2[coef_2 != 0].index.tolist()\n",
    "features_dropped_2 = coef_2[coef_2 == 0].index.tolist()\n",
    "\n",
    "print(\"Lasso chose \" + str(sum(coef_2 != 0)) + \" features and dropped \" +  str(sum(coef_2 == 0)) + \" features\")\n",
    "print('Features chosen', features_chosen_2)\n",
    "print('Features dropped', features_dropped_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_coef_2 = coef_2.sort_values()\n",
    "plt.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef_2.plot(kind = \"barh\")\n",
    "plt.title(\"Importance of variables using the Lasso Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see RIDGE doesn't drop any feature while LASSO drops 2 which are 'venomous' and 'domestic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLs9lpH9QhJv"
   },
   "source": [
    "#11. Using 2 components which weight has the 'domestic' feautre in component 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use PCA to check that\n",
    "\n",
    "# First we have to check the variance, if there's one that is very high we have to do something\n",
    "X.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As 'legs' has a high value over the others let's use StandardScaler on all of our data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# As we only want to see the weight we don't need to transform , just fitting we are going to be able to see what we need\n",
    "cargas = pd.DataFrame(pca.components_, columns=X.columns, index=['PC1', 'PC2'])\n",
    "print(f'Weight of the features in the principal components:\\n{cargas}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's filter to see what we exactly need to see\n",
    "peso_domestic_pc1 = cargas.loc['PC1', 'domestic']\n",
    "print(f'El peso de la variable domestic en la componente 1 es: {peso_domestic_pc1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to plot everything we can apply the following code (Change the title to your language, in this case the titles and labels are in Spanish)\n",
    "plt.figure(figsize=(10, 6))\n",
    "cargas.T.plot(kind='bar', figsize=(10, 6), width=0.8) # Se hace para transponer el dataframe y que las variables estén en el eje X y las componentes en el eje Y\n",
    "plt.title(\"Cargas de las variables en las componentes principales\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.ylabel(\"Cargas\")\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8) # Añade una línea en Y=0 para diferenciar entre las cargas positivas y negativas\n",
    "plt.legend(title=\"Componentes principales\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use a heatmap to visualize it\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(cargas, annot=True, cmap=\"coolwarm\", center=0, cbar_kws={'label': 'Cargas'})\n",
    "plt.title(\"Cargas de las variables en las componentes principales\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.ylabel(\"Componentes principales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To only plot the 'domestic' values we can do it like this\n",
    "pesos_domestic = cargas['domestic']\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(6, 4))\n",
    "pesos_domestic.plot(kind='bar', color=['blue', 'orange'], alpha=0.8)\n",
    "plt.title(\"Peso de la variable 'domestic' en las componentes principales\")\n",
    "plt.ylabel(\"Peso (carga)\")\n",
    "plt.xlabel(\"Componentes principales\")\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORyfbwI-RHZW"
   },
   "source": [
    "#12. How many coeficients do we obtain in LDA ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA is a supervised learning algorithm used for classification, but it is also utilized in dimensionality reduction tasks as it seeks the combination of features that best separates the classes \n",
    "# in a dataset.\n",
    "# To ensure this algorithm works correctly, we need a Gaussian distribution and similar covariance matrices.\n",
    "# In our case, if we observe X, it does not have a Gaussian distribution, but if we observe X_scaled, it does.\n",
    "\n",
    "sns.histplot(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coeficients\n",
    "coeficientes = pd.DataFrame(lda.coef_, columns=X.columns)\n",
    "print('Número de coeficientes:\\n', coeficientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we obtain 6 coefficients for each variable. This is due to how LDA calculates the coefficients, using the formula min(k-1, p), where k is the number of classes and p is the total number of variables. Thus, when we substitute the values (7-1 = 6), we get min(6, 16) = 6, resulting in 6 coefficients per variable.\n",
    "\n",
    "The absolute values of the coefficients indicate the importance of each variable for each discriminant. Therefore, the variables with the highest absolute values in each discriminant signify that those variables have a more significant impact on the discrimination between classes for that discriminant."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
